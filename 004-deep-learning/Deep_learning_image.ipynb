{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: middle;\" src=\"../images/logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "* The [Python 3 documentation]. For those unfamiliar with Python the [official tutorial] is recommended\n",
    "* The Software Carpentry [novice Python lessons]\n",
    "* [IPython's own notebook tutorial](http://nbviewer.jupyter.org/github/ipython/ipython/blob/3.x/examples/Notebook/Index.ipynb)\n",
    "* [Markdown cheatsheet] (Markdown is the syntax you use to write formatted text into cells in a notebook.)\n",
    "\n",
    "[Python 3 documentation]: https://docs.python.org/3/\n",
    "[official tutorial]: https://docs.python.org/3/tutorial/index.html\n",
    "[novice python lessons]: http://swcarpentry.github.io/python-novice-inflammation/\n",
    "[Markdown cheatsheet]: https://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet\n",
    "\n",
    "Often, there is a need to have a common analysis framework that allows the broad research community to use as a starting point for the analysis to establish baseline results that can be comparable across research groups, projects and technologies. Scanpy (Python) and Seurat (R) are such frameworks for single cell analysis. Importantly, it is necessary to understand the background theory and the low-level codes behind convenient wrapper functions.\n",
    "\n",
    "For spatial transcriptomic data, there are many analysis types and more than 100s of software programs are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 10x Genomics Breast Cancer Samples\n",
    "\n",
    "10x Genomics have made available samples from various tissue types, platforms and species. To get an idea of transcriptomic data and available analysis techniques, [we're going to use Visium datasets of human breast cancer samples](https://www.10xgenomics.com/datasets?query=&page=1&configure%5BhitsPerPage%5D=50&configure%5BmaxValuesPerFacet%5D=1000&refinementList%5Bplatform%5D%5B0%5D=Visium%20Spatial&refinementList%5BanatomicalEntities%5D%5B0%5D=breast).\n",
    "\n",
    "For understanding about the Visium data, you could consult the lecture note. A three-minute introduction video can be found [here](https://www.youtube.com/watch?v=VwNk4d-0RJc). The diagram below explains the concept\n",
    "![Visium Gene Expression Slide](https://ngisweden.scilifelab.se/wp-content/uploads/2021/02/Visium-gene-expression-slide-1024x390.png)\n",
    "\n",
    "Visium data sets include:\n",
    "* Images of the slides,\n",
    "* Spatial location of spots, 55 ùúám spot diameter and 100 ùúám center to center, used to sample the transcriptome, and\n",
    "* Gene expression counts, 1-10% of the total mRNA molecules present in a given spot, of the full transcriptome (~20,000 genes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import anndata\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as matplotlib_pyplot\n",
    "import numpy as numpy\n",
    "import pandas as pandas\n",
    "import scanpy as scanpy\n",
    "import scipy as scipy\n",
    "import sklearn.cluster as sklearn_cluster\n",
    "import sklearn.decomposition as sklearn_decomposition\n",
    "import sklearn.model_selection as sklearn_model_selection\n",
    "import sklearn.preprocessing as sklearn_preprocessing\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tf_keras\n",
    "import tensorflow.keras.applications as tf_keras_applications\n",
    "import tensorflow.keras.layers as tf_keras_layers\n",
    "import tensorflow.keras.models as tf_keras_models\n",
    "import umap\n",
    "from requests.packages import target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup DPI and image output\n",
    "dpi = 96\n",
    "matplotlib.rcParams['figure.dpi'] = dpi\n",
    "scanpy.settings.set_figure_params(dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formating for the notebook\n",
    "# Suppress warnings for tidy representation of the notebook   \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    \n",
    "def print_msg(msg_lst: list, sep='\\t', color='\\033[91m'):\n",
    "    msg = \"\"\n",
    "    for i in msg_lst:\n",
    "        msg += str(i) + sep\n",
    "\n",
    "    print(color + \"-\" * 80 + bcolors.ENDC)\n",
    "    print(color + msg.center(80, ' ') + bcolors.ENDC)\n",
    "    print(color + '-' * 80 + bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify PATH to data\n",
    "# Breast Cancer Sample from 10x Genomics\n",
    "data_path = pathlib.Path(\"../data\")\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "results_path = pathlib.Path(\"../results\")\n",
    "results_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# The files are available at https://www.10xgenomics.com/datasets/human-breast-cancer-block-a-section-1-1-standard-1-1-0\n",
    "# Specifically:\n",
    "# * https://cf.10xgenomics.com/samples/spatial-exp/1.1.0/V1_Breast_Cancer_Block_A_Section_1/V1_Breast_Cancer_Block_A_Section_1_spatial.tar.gz\n",
    "# * https://cf.10xgenomics.com/samples/spatial-exp/1.1.0/V1_Breast_Cancer_Block_A_Section_1/V1_Breast_Cancer_Block_A_Section_1_filtered_feature_bc_matrix.tar.gz\n",
    "library_id = 'V1_Breast_Cancer_Block_A_Section_1'\n",
    "ann_data = scanpy.datasets.visium_sge(sample_id=library_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = ann_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Visualising Counts and Genes\n",
    "\n",
    "<span style=\"color:blue\">**Example problem**: Calculate the total counts (or reads) for each spot and store in '.obs['total_counts']. Next, calculate the number of genes detected (non-zero counts) and store in '.obs['n_genes_by_counts']. Then visualise the results, what do they mean?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_data.obs['total_counts'] = ann_data.X.sum(axis=1)\n",
    "ann_data.obs['n_genes_by_counts'] = (ann_data.X > 0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise metrics\n",
    "scanpy.pl.spatial(ann_data, img_key=\"lowres\", color=[\"total_counts\", \"n_genes_by_counts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise H&E Image\n",
    "\n",
    "A H&E ([Hematoxylin](https://en.wikipedia.org/wiki/Haematoxylin) and [Eosin](https://en.wikipedia.org/wiki/Eosin)) stain uses two dyes to stain a tissue section and provides important information about its structure. Hematoxylin stains cell nuclei blue-purple and eosin stains the cytoplasm and extracellular matrix in various shades of pink. Taken separately to the other data, it can be challenging to align it. [Fiducials](https://www.10xgenomics.com/support/software/space-ranger/latest/analysis/inputs/image-fiducial-alignment) are reference markers or points used for calibration, alignment, and orientation in imaging and spatial technologies. In spatial transcriptomics, they serve as fixed points of reference to align different types of data or images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanpy.pl.spatial(ann_data, color=None, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Filtering and Normalisation\n",
    "\n",
    "In a spatial dataset, each spot has its own spatial barcode, and genes are measured separately for each spot. Due to experimental limitations, biases can occur, where some spots receive more sequencing information than others, leading to variation in the total number of reads per spot. To address this technical variation, spot-based normalization is applied.\n",
    "\n",
    "#### 2.1. Filtering\n",
    "\n",
    "Filtering steps are commonly used in single-cell and spatial transcriptomics data preprocessing to help reduce these biases, technical noise reduction and to improved downstream processing results (normalisation, dimension reduction, differernt gene analysis, etc).\n",
    "\n",
    "The following is relatively conservative and a better approach might be to observe the distribution of the data first and adjust values accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_data.var_names_make_unique()\n",
    "scanpy.pp.filter_genes(ann_data, min_cells=3)\n",
    "scanpy.pp.filter_cells(ann_data, min_genes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Normalisation\n",
    "\n",
    "Normalisation is an important step to address the challenges of varying RNA content and sequencing depth across different spots in a tissue section. Median normalisation adjusts gene expression values by scaling each spot's counts to a common median, effectively accounting for technical variations while preserving biological differences.\n",
    "\n",
    "To achieve this, you need to calculate the total number of reads (or UMIs) for each spot and calculate the median across all spots in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Visualising the Effects of Normalisation\n",
    "\n",
    "We can observe the effect of the normalisation step. Due to spots having very similar values (near identical), the color scale bar is limited.\n",
    "\n",
    "Here, we use the default normalization option in Scanpy, called \"normalize by total.\" After normalization, each cell's total count is scaled to the median of the total counts across all cells before normalization. While this method is fast, it may be less accurate in certain biological scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise before Normalisation\n",
    "scanpy.pl.spatial(ann_data, img_key=\"lowres\", color=[\"total_counts\", \"n_genes_by_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize and log transform data\n",
    "scanpy.pp.normalize_total(ann_data, target_sum=1e4)\n",
    "scanpy.pp.log1p(ann_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update counts and number of genes\n",
    "ann_data.obs['total_counts'] = ann_data.X.sum(axis=1)\n",
    "ann_data.obs['n_genes_by_counts'] = (ann_data.X > 0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise After\n",
    "scanpy.pl.spatial(ann_data, img_key=\"lowres\", color=[\"total_counts\", \"n_genes_by_counts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Networks\n",
    "\n",
    "Neural networks are a type of machine learning model inspired by the structure and function of the human brain. They consist of interconnected nodes, or \"neurons,\" organized in layers that process and transmit information.\n",
    "Training data plays a crucial role in developing effective neural networks. It consists of labeled examples that the network uses to learn patterns and relationships, adjusting its internal parameters through a process called backpropagation to improve its performance on the given task.\n",
    "\n",
    "To setup our neural network, first we're preparing the training and testing data sets: 70% of the data (x_train) is used for training, while 30% (x_test) is reserved for testing (evaluation). Later we will also assess KFold cross validation design to understand how random sampling can be used to train a neural network model (this part is optional).\n",
    "\n",
    "An important task for a neural network is to learn features in the data. In the original data space, each gene represents a dimension. Through Neural Network (e.g., Autoencoder or Resnet50 as you will apply in this tutorial) we can find higher representation of the cell/spot in latent/reduced dimension space, where each dimension can be computed but is unknown in the context of which genes or which information they represent (e.g., a principal component in PCA or a vector of neurons in Autoencoder or Resnet50 outputs).  The latent dimensions (or hidden layer in the neural network architecture) is set to 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data\n",
    "nn_raw_data = ann_data.to_df()\n",
    "nn_raw_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare inputs for the neural network, splitting the data to training set (70%) and test set (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = sklearn_model_selection.train_test_split(nn_raw_data, test_size=0.3, random_state=1)\n",
    "feature_dim = len(nn_raw_data.columns)\n",
    "latent_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions of x_train:\", x_train.shape)\n",
    "print(\"Dimensions of x_test:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim and Approach\n",
    "\n",
    "Aim: Integrating imaging features and sequencing features to latent space \n",
    "\n",
    "Approach: we will run three steps and explore two neural networks\n",
    "1. Extract gene expression features using an autoencoder\n",
    "2. Extract imaging features using CNN and transfer learning\n",
    "3. Perform clustering and compare results from imaging alone, gene expression alone and the combination of both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Autoencoders\n",
    "![Basic Diagram for an Autoencoder from Towards AI](../images/autoencoder.png)\n",
    "\n",
    "From: [\"Let‚Äôs Talk Auto-Encoders\"](https://pub.towardsai.net/lets-talk-auto-encoders-d29e9a94e994)\n",
    "\n",
    "An autoencoder is an unsupervised, neural network architecture. An encoder compresses the input data into a lower-dimensional representation called the \"latent space\" or \"bottleneck\". The decoder attempts to reconstruct the original input from this compressed representation. The bottleneck layer learns the most important features of the data, effectively creating a compact, informative representation. By comparing the reconstructed output to the original input, the autoencoder can be trained to minimise the reconstruction error, ensuring that the latent space captures the essential characteristics of the data while filtering out noise and less important details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.1  Model Architecture\n",
    "\n",
    "In the code below, feature_dim represents the number of input features, determined by counting the columns in the original dataset (nn_raw_data) - in this case the number of genes. Latent_dim is the number of reduced dimensions the model will learn to represent the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAutoencoder(tf_keras_models.Model):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(SpatialAutoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = tf_keras_models.Sequential([\n",
    "            tf_keras_layers.InputLayer(input_shape=(input_dim,)),\n",
    "            tf_keras_layers.Dense(128, activation='relu'),\n",
    "            tf_keras_layers.Dense(64, activation='relu'),\n",
    "            tf_keras_layers.Dense(latent_dim, activation='relu')\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf_keras_models.Sequential([\n",
    "            tf_keras_layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            tf_keras_layers.Dense(64, activation='relu'),\n",
    "            tf_keras_layers.Dense(128, activation='relu'),\n",
    "            tf_keras_layers.Dense(input_dim, activation='linear')\n",
    "        ])\n",
    "\n",
    "        # Create the full autoencoder model\n",
    "        input_layer = tf_keras_layers.Input(shape=(input_dim,))\n",
    "        encoded = self.encoder(input_layer)\n",
    "        decoded = self.decoder(encoded)\n",
    "        self.model = tf_keras_models.Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def get_latent_representation(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Neural Network, the loss function (also called objective function) is very important to optimise the model parameters in a way that it can perform the prediction most accurately by minimizing the loss between the expected values and the predicted values. Different loss functions have their own effects in how the model learns. For example, Mean Squared Error is suitable for a regression task while entropy loss is suitable for classification. Mean Squared Error is not as sensitive as Mean Absolute Error in detecting small difference. \n",
    "\n",
    "The mean squared error (MSE) is a common metric used to measure the average squared difference between the predicted values (y_hat) and the actual values (y) in a regression problem. It is calculated by taking the sum of the squared differences and dividing it by the number of samples (n):\n",
    "\n",
    "MSE = (1/n) * sum((y_i - y_hat_i)^2)\n",
    "\n",
    "The mean absolute error (MAE) is another common metric used to measure the average absolute difference between the predicted values (y_hat) and the actual values (y) in a regression problem. It is calculated by taking the sum of the absolute differences and dividing it by the number of samples (n):\n",
    "\n",
    "MAE = (1/n) * sum(|y_i - y_hat_i|)\n",
    "\n",
    "where:\n",
    "- n is the number of samples\n",
    "- y_i is the actual value of the i-th sample\n",
    "- y_hat_i is the predicted value of the i-th sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf_keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the optimization of a neural network, gradient descend and back propagration are used (as explained in the lectorials). In the code, we are using the popular Adam optimizer. An introduction to Adam optimizer is here  more information, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = SpatialAutoencoder(feature_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different learning rates - you could consult the lecture note for the effect of learning rate.\n",
    "# autoencoder.compile(optimizer=tf_keras.optimizers.Adam(learning_rate=1e-3), loss=loss_fn)\n",
    "autoencoder.compile(optimizer=tf_keras.optimizers.Adam(learning_rate=1e-5), loss=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**Example problem**: What is the total number of trainable parameters?</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a summary of the model\n",
    "autoencoder.encoder.summary()  # total params\n",
    "\n",
    "\n",
    "def get_trainable_params(model):\n",
    "    total_params = 0\n",
    "    for layer in model.layers:\n",
    "        for weight in layer.trainable_weights:\n",
    "            total_params += tf.size(weight).numpy()\n",
    "    return total_params\n",
    "\n",
    "\n",
    "trainable_params = get_trainable_params(autoencoder.encoder)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Training the Autoencoder Model \n",
    "\n",
    "For the actual training of the model, we define hyper parameters such as the number of epochs (the number of iterations), the batch_size (the number of data points to be used in each parallel training. The size of the minibatch is often limited by the RAM of the GPU), and whether or not the entire training data is randomly shuffled before each epoch (to avoid reliance on the order of the data and so to improve generalization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = autoencoder.fit(x_train, x_train,\n",
    "                                   epochs=15,\n",
    "                                   batch_size=16,\n",
    "                                   shuffle=True,\n",
    "                                   validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Examining Loss\n",
    "\n",
    "### Plotting Total Loss\n",
    "\n",
    "A good way to assess if the model is learning and if overfitting or underfitting occurs is through visualising the loss values across epoches and comparing the changes for training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_pyplot.plot(training_history.history[\"loss\"], label=\"Training Loss\")\n",
    "matplotlib_pyplot.plot(training_history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "matplotlib_pyplot.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Loss per Spot\n",
    "\n",
    "Above, we plot the total loss across the whole dataset. Here, we can also visualise loss per spot, as each spot receives its own predicted value. We also compare MSE and MAE loss as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Get the reconstructed outputs\n",
    "reconstructed = autoencoder.predict(x_test)\n",
    "\n",
    "# Calculate reconstruction error for each sample\n",
    "mse_per_sample = numpy.mean((x_test - reconstructed) ** 2, axis=1)\n",
    "mae_per_sample = numpy.mean(numpy.abs(x_test - reconstructed), axis=1)\n",
    "\n",
    "# Print overall reconstruction error\n",
    "overall_mse = mean_squared_error(x_test.values.flatten(), reconstructed.flatten())\n",
    "overall_mae = mean_absolute_error(x_test.values.flatten(), reconstructed.flatten())\n",
    "\n",
    "print(f\"Overall Mean Squared Error: {overall_mse}\")\n",
    "print(f\"Overall Mean Absolute Error: {overall_mae}\")\n",
    "\n",
    "# Plot the reconstruction errors\n",
    "matplotlib_pyplot.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot MSE\n",
    "matplotlib_pyplot.subplot(1, 2, 1)\n",
    "matplotlib_pyplot.plot(mse_per_sample, label='MSE per sample')\n",
    "matplotlib_pyplot.xlabel('Sample Index')\n",
    "matplotlib_pyplot.ylabel('MSE')\n",
    "matplotlib_pyplot.title('Mean Squared Error per Sample')\n",
    "matplotlib_pyplot.legend()\n",
    "\n",
    "# Plot MAE\n",
    "matplotlib_pyplot.subplot(1, 2, 2)\n",
    "matplotlib_pyplot.plot(mae_per_sample, label='MAE per sample')\n",
    "matplotlib_pyplot.xlabel('Sample Index')\n",
    "matplotlib_pyplot.ylabel('MAE')\n",
    "matplotlib_pyplot.title('Mean Absolute Error per Sample')\n",
    "matplotlib_pyplot.legend()\n",
    "\n",
    "matplotlib_pyplot.tight_layout()\n",
    "matplotlib_pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Plotting Gene Expression Reconstruction from the Trained Autoencoder\n",
    "\n",
    "After training the autoencoder, we have minimised the cost and are now able to reconstruct the expression pattern, where the reduced latent space should be able to be decoded to predict corresponding values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming autoencoder, nn_raw_data, and ann_data are already defined\n",
    "# Predict using Autoencoder\n",
    "predicted_data = autoencoder.predict(ann_data.X)\n",
    "print(nn_raw_data.shape)\n",
    "print(predicted_data.shape)\n",
    "print(ann_data.X.shape)\n",
    "\n",
    "# Ensure the predicted data has the correct shape\n",
    "if predicted_data.shape != ann_data.X.shape:\n",
    "    raise ValueError(\n",
    "        f\"Shape mismatch: predicted_data has shape {predicted_data.shape}, but ann_data.X has shape {ann_data.X.shape}.\")\n",
    "\n",
    "# Write predicted_data to the X slot in ann_data\n",
    "ann_data.X = predicted_data\n",
    "\n",
    "# Ensure the predicted data has the correct shape before flattening\n",
    "if predicted_data.shape[0] != ann_data.obs.shape[0]:\n",
    "    raise ValueError(\n",
    "        f\"Length mismatch: predicted_data has length {predicted_data.shape[0]}, but ann_data.obs has length {ann_data.obs.shape[0]}.\")\n",
    "\n",
    "# Add nn_raw_data and predictions to ann_data.obs for visualization\n",
    "ann_data.obs['predicted'] = predicted_data.mean(axis=1)\n",
    "ann_data.obs['nn_raw'] = nn_raw_data.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot nn_raw_data and predictions on tissue\n",
    "scanpy.pl.spatial(ann_data, img_key=\"lowres\", color=[\"nn_raw\", \"predicted\"], title=\"nn_raw_data on Tissue\",\n",
    "                  palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Testing on Unseen Datasets\n",
    "\n",
    "The reconstruction in the session 1.4 above works well. However, one may question that the successful reconstruction is because the model was trained and used to predict the same dataset. This is a valid question, and so to address that we can try to use the trained autoencoder and test a different dataset. Here we will use a second breast cancer Visium dataset that is a replicate of the sample used in 1.4, but measured independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# The files are available at https://cf.10xgenomics.com/samples/spatial-exp/1.1.0/V1_Breast_Cancer_Block_A_Section_2/\n",
    "# Specifically:\n",
    "# * https://cf.10xgenomics.com/samples/spatial-exp/1.1.0/V1_Breast_Cancer_Block_A_Section_2/V1_Breast_Cancer_Block_A_Section_2_spatial.tar.gz\n",
    "# * https://cf.10xgenomics.com/samples/spatial-exp/1.1.0/V1_Breast_Cancer_Block_A_Section_2/V1_Breast_Cancer_Block_A_Section_2_filtered_feature_bc_matrix.h5\n",
    "\n",
    "ann_data_rep2 = scanpy.datasets.visium_sge(sample_id='V1_Breast_Cancer_Block_A_Section_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the data requires a h5 file and a spatial folder located in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the AnnData object to confirm successful loading\n",
    "print(f\"AnnData object created with shape: {ann_data_rep2.shape}\")\n",
    "\n",
    "# Visualize the spatial data\n",
    "scanpy.pl.spatial(ann_data_rep2, color=None, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_data_rep2.obs['total_counts'] = ann_data_rep2.X.sum(axis=1)\n",
    "ann_data_rep2.obs['n_genes_by_counts'] = (ann_data_rep2.X > 0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check general information about the AnnData object\n",
    "ann_data_rep2.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess this dataset in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2 = ann_data_rep2.copy()\n",
    "ann_data_rep2.var_names_make_unique()\n",
    "scanpy.pp.filter_genes(ann_data_rep2, min_cells=3)\n",
    "scanpy.pp.filter_cells(ann_data_rep2, min_genes=10)\n",
    "scanpy.pl.spatial(ann_data_rep2, img_key=\"lowres\", color=[\"total_counts\", \"n_genes_by_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the input data\n",
    "print(f\"Shape of ann_data_rep2.X: {ann_data_rep2.X.shape}\")\n",
    "\n",
    "# Print the expected input shape of the autoencoder\n",
    "expected_input_shape = autoencoder.model.input_shape[1:]  # Exclude the batch size\n",
    "print(f\"Expected input shape of the autoencoder: {expected_input_shape}\")\n",
    "\n",
    "# Ensure the data is of type float32\n",
    "dense_data = ann_data_rep2.X.toarray() if not isinstance(ann_data_rep2.X, numpy.ndarray) else ann_data_rep2.X\n",
    "if dense_data.dtype != numpy.float32:\n",
    "    dense_data = dense_data.astype(numpy.float32)\n",
    "\n",
    "# Adjust the input data shape if necessary\n",
    "if dense_data.shape[1] != expected_input_shape[0]:\n",
    "    print(f\"Adjusting input data shape from {dense_data.shape[1]} to {expected_input_shape[0]}\")\n",
    "    if dense_data.shape[1] > expected_input_shape[0]:\n",
    "        dense_data = dense_data[:, :expected_input_shape[0]]  # Truncate\n",
    "    else:\n",
    "        padding = expected_input_shape[0] - dense_data.shape[1]\n",
    "        dense_data = numpy.pad(dense_data, ((0, 0), (0, padding)), 'constant')  # Pad with zeros\n",
    "\n",
    "# Predict using the autoencoder\n",
    "predicted_data = autoencoder.model.predict(dense_data)\n",
    "print(f\"Shape of predicted_data: {predicted_data.shape}\")\n",
    "\n",
    "# Add nn_raw_data and predictions to ann_data_rep2.obs for visualization\n",
    "ann_data_rep2.obs['nn_raw'] = ann_data_rep2.X.mean(axis=1)\n",
    "ann_data_rep2.obs['predicted'] = predicted_data.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot nn_raw and predicted on tissue\n",
    "scanpy.pl.spatial(ann_data_rep2, img_key=\"lowres\", color=[\"nn_raw\", \"predicted\"], title=\"nn_raw_data on Tissue\",\n",
    "                  palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. K-Fold Cross-Validation (Optional)\n",
    "\n",
    "While training the autoencoder, we have assessed visually the loss in the train dataset and test dataset to see if the model is learning. Each time of random sampling would yield different results. We can assess the effect of this randomness using KFold cross validation. In many cases, after KFold runs, we can select the best performance model out of the K models to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_raw_data_np = nn_raw_data.to_numpy()\n",
    "kf = sklearn_model_selection.KFold(n_splits=3)\n",
    "fold_no = 1\n",
    "training_loss_per_fold = []\n",
    "validation_loss_per_fold = []\n",
    "autoencoder.compile(optimizer=tf_keras.optimizers.Adam(learning_rate=1e-3), loss=loss_fn)\n",
    "for train_index, test_index in kf.split(nn_raw_data_np):\n",
    "    x_train_cv, x_test_cv = nn_raw_data_np[train_index], nn_raw_data_np[test_index]\n",
    "    # Train the autoencoder\n",
    "    training_history = autoencoder.fit(x_train_cv, x_train_cv,\n",
    "                                       epochs=15,\n",
    "                                       batch_size=16,\n",
    "                                       shuffle=True,\n",
    "                                       validation_data=(x_test_cv, x_test_cv))\n",
    "    # Store training and validation loss\n",
    "    training_loss_per_fold.append(training_history.history['loss'])\n",
    "    validation_loss_per_fold.append(training_history.history['val_loss'])\n",
    "    fold_no += 1\n",
    "    autoencoder.compile(optimizer=tf_keras.optimizers.Adam(learning_rate=1e-3), loss=loss_fn)\n",
    "    # Plot the training and validation loss for each fold\n",
    "matplotlib_pyplot.figure(figsize=(12, 8))\n",
    "for i in range(len(training_loss_per_fold)):\n",
    "    matplotlib_pyplot.plot(training_loss_per_fold[i], label=f'Training Loss Fold {i + 1}')\n",
    "    matplotlib_pyplot.plot(validation_loss_per_fold[i], label=f'Validation Loss Fold {i + 1}', linestyle='--')\n",
    "matplotlib_pyplot.title('Training and Validation Loss per Epoch for Each Fold')\n",
    "matplotlib_pyplot.xlabel('Epoch')\n",
    "matplotlib_pyplot.ylabel('Loss')\n",
    "matplotlib_pyplot.legend()\n",
    "matplotlib_pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extracting Features from Imaging Data with Transfer Learning\n",
    "\n",
    "Above, we use autoencoder to extract gene expression data. However, the imaging information is also important for cancer diagnosis. In fact, assessment of histological images by pathologists are used as gold standard diagnosis practice for many years (>100 years). Therefore, we now have the opportunity to incorporate the imaging information (what the eyes can see) with gene expression information (the eyes can not see) to make the best use of all information from a precious patient tissue sample. Below we will go through the steps to take.\n",
    "\n",
    "### 4.1. Tiling Images\n",
    "\n",
    "Images are often too large to conveniently process. A common approach in computer vision is to split (tile) the image and make use of the highly parallel computation to train the tiles. Notably, we also have corresponding locations of each spot that we measure the gene expression. To combine imaging information with the gene expression information for each spot, we will extract the tile image correspond to spot locations rather than using random tiling.\n",
    "\n",
    "The following code require some understanding of the high resolution image vs low resolution image and scaling factors to convert the two resolutions and corresponding spot coordinates. This information is not essential so you could ignore the details in the code block below, just appreciate the concept that we extract the tiles based on their coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load high-resolution image \n",
    "highres_image = ann_data.uns['spatial'][library_id]['images']['hires']\n",
    "# display the high-resolution image\n",
    "matplotlib.pyplot.imshow(highres_image)\n",
    "matplotlib.pyplot.axis('off')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the spot coordinates to match the image dimensions\n",
    "spot_coords = ann_data.obsm['spatial']\n",
    "scalefactor = ann_data.uns['spatial'][library_id]['scalefactors']['tissue_hires_scalef']\n",
    "spot_coords_scaled = spot_coords * scalefactor\n",
    "\n",
    "# Function to extract tiles based on spot coordinates with boundary checks\n",
    "def extract_tiles(image, coords, tile_size):\n",
    "    tiles = []\n",
    "    img_height, img_width, _ = image.shape\n",
    "    half_tile = tile_size // 2\n",
    "    for coord in coords:\n",
    "        x, y = coord\n",
    "        x_start = max(0, int(x - half_tile))\n",
    "        y_start = max(0, int(y - half_tile))\n",
    "        x_end = min(img_width, x_start + tile_size)\n",
    "        y_end = min(img_height, y_start + tile_size)\n",
    "\n",
    "        # Adjust start positions if the tile is smaller than the desired size\n",
    "        if x_end - x_start < tile_size:\n",
    "            x_start = x_end - tile_size\n",
    "        if y_end - y_start < tile_size:\n",
    "            y_start = y_end - tile_size\n",
    "\n",
    "        tile = image[y_start:y_end, x_start:x_end]\n",
    "        tiles.append(tile)\n",
    "    return numpy.array(tiles)\n",
    "\n",
    "\n",
    "# Define the tile size (adjust based on your data)\n",
    "tile_size = 224  # Example tile size\n",
    "\n",
    "# Extract tiles based on scaled spot coordinates\n",
    "tiles = extract_tiles(highres_image, spot_coords_scaled, tile_size)\n",
    "\n",
    "# Check the spot coordinates\n",
    "print(\"Spot coordinates (first 10):\", spot_coords[:10])\n",
    "print(\"Scaled spot coordinates (first 10):\", spot_coords_scaled[:10])\n",
    "\n",
    "# Display some of the tiles\n",
    "n = 3  # Number of tiles to display\n",
    "matplotlib_pyplot.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = matplotlib_pyplot.subplot(1, n, i + 1)\n",
    "    matplotlib_pyplot.imshow(tiles[i])\n",
    "    matplotlib_pyplot.axis('off')\n",
    "matplotlib_pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Transfer Learning\n",
    "\n",
    "Transfer learning is a powerful tool in machine learning, where existing models have been trained on similar data or large, universal data can then be utilised in our new model. Here we use a model that has been trained on over one million images in the ImageNet dataset. The trained model is ResNet50. We can use this model to extract our image tiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image tiles\n",
    "def preprocess_image(image, target_size=(224, 224)):\n",
    "    image = tf.image.resize(image, target_size)\n",
    "    image = tf_keras_applications.resnet50.preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Preprocess each tile\n",
    "tiles_preprocessed = numpy.array([preprocess_image(tile) for tile in tiles])\n",
    "\n",
    "# Add batch dimension\n",
    "tiles_preprocessed = tf.convert_to_tensor(tiles_preprocessed, dtype=tf.float32)\n",
    "\n",
    "# Load ResNet50 model without the top layer\n",
    "base_model = tf_keras_applications.ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = tf_keras_layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a fully connected layer with 128 neurons\n",
    "x = tf_keras_layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "# Create the model\n",
    "resnet_model = tf_keras_models.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Extract features from the tiles\n",
    "resnet_features = resnet_model.predict(tiles_preprocessed)\n",
    "\n",
    "print(\"Extracted ResNet50 features shape:\", resnet_features.shape)\n",
    "print(\"Extracted ResNet50 features:\", resnet_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Clustering Based on Image Features\n",
    "\n",
    "\n",
    "#### 4.3.1. Leiden Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an AnnData object from the combined features\n",
    "resnet_features_ann_data = anndata.AnnData(X=resnet_features)\n",
    "\n",
    "# Perform compute nearest-neighbours index, leiden clustering, UMAP\n",
    "scanpy.pp.neighbors(resnet_features_ann_data, n_neighbors=10, use_rep='X', random_state=42)\n",
    "scanpy.tl.umap(resnet_features_ann_data)\n",
    "scanpy.tl.leiden(resnet_features_ann_data, resolution=0.7, n_iterations=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an AnnData object from the combined features\n",
    "resnet_features_ann_data = anndata.AnnData(X=resnet_features)\n",
    "\n",
    "# Perform compute nearest-neighbours index, leiden clustering, UMAP\n",
    "scanpy.pp.neighbors(resnet_features_ann_data, n_neighbors=10, use_rep='X', random_state=42)\n",
    "scanpy.tl.umap(resnet_features_ann_data)\n",
    "scanpy.tl.leiden(resnet_features_ann_data, resolution=0.7, n_iterations=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display UMAP of features\n",
    "scanpy.pl.umap(resnet_features_ann_data, color='leiden', cmap='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract cluster labels\n",
    "cluster_labels = resnet_features_ann_data.obs['leiden'].astype(int).to_numpy()\n",
    "highres_image = ann_data.uns['spatial'][library_id]['images']['hires']\n",
    "spot_coords = ann_data.obsm['spatial']\n",
    "\n",
    "# Scale the spot coordinates to match the image dimensions\n",
    "scalefactor = ann_data.uns['spatial'][library_id]['scalefactors']['tissue_hires_scalef']\n",
    "spot_coords_scaled = spot_coords * scalefactor\n",
    "\n",
    "# Plot clusters on tissue\n",
    "matplotlib_pyplot.figure(figsize=(10, 10))\n",
    "matplotlib_pyplot.imshow(highres_image, extent=[0, highres_image.shape[1], highres_image.shape[0], 0])\n",
    "for i in numpy.unique(cluster_labels):\n",
    "    cluster_spots = spot_coords_scaled[cluster_labels == i]\n",
    "    matplotlib_pyplot.scatter(cluster_spots[:, 0], cluster_spots[:, 1], label=f'Cluster {i}', s=10)\n",
    "matplotlib_pyplot.legend()\n",
    "matplotlib_pyplot.title('Clusters on Tissue')\n",
    "matplotlib_pyplot.axis('off')\n",
    "matplotlib_pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanpy.pp.pca(ann_data, n_comps=50)\n",
    "scanpy.pp.neighbors(ann_data, n_neighbors=20, random_state=42)\n",
    "scanpy.tl.umap(ann_data)\n",
    "scanpy.tl.leiden(ann_data, resolution=0.5, n_iterations=-1)\n",
    "scanpy.pl.spatial(ann_data, img_key=\"lowres\", color=\"leiden\", title=\"Spatial distribution of Leiden clusters\",\n",
    "                  palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2. K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "n_clusters = 6  # Adjust the number of clusters as needed\n",
    "kmeans = sklearn_cluster.KMeans(n_clusters=n_clusters, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(resnet_features)\n",
    "\n",
    "highres_image = ann_data.uns['spatial'][library_id]['images']['hires']\n",
    "spot_coords = ann_data.obsm['spatial']\n",
    "\n",
    "# Scale the spot coordinates to match the image dimensions\n",
    "scalefactor = ann_data.uns['spatial'][library_id]['scalefactors']['tissue_hires_scalef']\n",
    "spot_coords_scaled = spot_coords * scalefactor\n",
    "\n",
    "# Plot clusters on tissue\n",
    "matplotlib_pyplot.figure(figsize=(10, 10))\n",
    "matplotlib_pyplot.imshow(highres_image, extent=[0, highres_image.shape[1], highres_image.shape[0], 0])\n",
    "for i in range(n_clusters):\n",
    "    cluster_spots = spot_coords_scaled[cluster_labels == i]\n",
    "    matplotlib_pyplot.scatter(cluster_spots[:, 0], cluster_spots[:, 1], label=f'Cluster {i}', s=10)\n",
    "matplotlib_pyplot.legend()\n",
    "matplotlib_pyplot.title('Clusters on Tissue')\n",
    "matplotlib_pyplot.axis('off')\n",
    "matplotlib_pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integrating Imaging and Sequencing Data\n",
    "\n",
    "We can combine two key components of the data, namely tissue imaging and gene expression value. Each of our spot now has a latent representation from the autoencoder (for gene expression) and another representation from Resnet50 (for imaging data). We can then concatenate these two latent vectors (for each spot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent space from the autoencoder\n",
    "latent_space = autoencoder.encoder.predict(nn_raw_data)\n",
    "\n",
    "print(\"Latent space shape:\", latent_space.shape)\n",
    "print(\"Latent space:\", latent_space)\n",
    "\n",
    "# Combine the features\n",
    "combined_features = numpy.concatenate((resnet_features, latent_space), axis=1)\n",
    "\n",
    "print(\"Combined features shape:\", combined_features.shape)\n",
    "print(\"Combined features:\", combined_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined latent space now has information from both gene expression and imaging information. We can then perform clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming combined_features and ann_data are already defined\n",
    "# Standardize the features\n",
    "scaler = sklearn_preprocessing.StandardScaler()\n",
    "combined_features_scaled = scaler.fit_transform(combined_features)\n",
    "\n",
    "# Perform UMAP\n",
    "umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "umap_embedding = umap_reducer.fit_transform(combined_features_scaled)\n",
    "\n",
    "# Convert UMAP embedding to AnnData object for Leiden clustering\n",
    "combined_ann_data = scanpy.AnnData(combined_features_scaled)\n",
    "combined_ann_data.obsm['X_umap'] = umap_embedding\n",
    "\n",
    "# Compute the neighborhood graph\n",
    "scanpy.pp.neighbors(combined_ann_data, n_neighbors=15, use_rep='X')\n",
    "\n",
    "# Perform Leiden clustering\n",
    "scanpy.tl.leiden(combined_ann_data, resolution=0.7, n_iterations=-1)\n",
    "\n",
    "# Add Leiden clusters to ann_data\n",
    "ann_data.obs['leiden_integrated'] = combined_ann_data.obs['leiden'].values\n",
    "\n",
    "# Plot UMAP with Leiden clusters\n",
    "matplotlib_pyplot.figure(figsize=(10, 8))\n",
    "scanpy.pl.umap(combined_ann_data, color='leiden', show=False)\n",
    "matplotlib_pyplot.title('UMAP projection with Leiden Clusters')\n",
    "matplotlib_pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine clustering results on tissue. We can compare clustering of just using image as shown in 2.3.1 above vs combining image with the gene expression here and vs original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanpy.pl.spatial(ann_data, img_key=\"lowres\", color=[\"leiden\", \"leiden_integrated\"], title=[\"Original Leiden Clusters\", \"Integrated Leiden Clusters\"],\n",
    "                  palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stimage2_ld",
   "language": "python",
   "name": "stimage2_ld"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
